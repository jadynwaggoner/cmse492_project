{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All imports used throughout testing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import lightkurve as lk\n",
    "import astropy.units as u\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function separates a pandas dataframe into separate smaller ones by variable star type using the 'Type' column from VisieR https://vizier.cds.unistra.fr/viz-bin/VizieR-3\n",
    "#The types focused on for this project are 'MISC' (miscellaneous), 'CST' (constant), 'RR' (RR Lyrae), 'EC' (contact eclipsing binary system), 'ED' (detached eclipsing binary system), 'DSCT' (delta scuti), 'DCEP' (classical cepheid/delta cephei-type variables), 'GDOR' (gamma doradus)\n",
    "#Subtypes listed in \"VARIABLE STAR TYPE DESIGNATIONS IN VSX\" https://www.aavso.org/vsx/index.php?view=about.vartypes are included in these general categories\n",
    "\n",
    "def septypes(dataset):  #Input a pandas dataframe that includes a 'Type' column from VisieR\n",
    "    nmisc = []  #Initializing empty lists where the indices where there is not a specific star type will be appended, for masking\n",
    "    ncst = []\n",
    "    nrr = []\n",
    "    ned = []\n",
    "    nec = []\n",
    "    ndsct = []\n",
    "    ndcep = []\n",
    "    ngdor = []\n",
    "\n",
    "    for i in range(len(dataset['Type'])):     #Loops through each index in the dataset\n",
    "        if 'MISC' not in dataset['Type'][i]:   #If a specific star type or any of its subtypes are not found in the 'Type',the index will be appended to the corresponding list\n",
    "            nmisc.append(i)\n",
    "\n",
    "        if 'CST' not in dataset['Type'][i]:\n",
    "            ncst.append(i)\n",
    "\n",
    "        if 'RR' not in dataset['Type'][i]:\n",
    "            nrr.append(i)\n",
    "\n",
    "        if 'EC' not in dataset['Type'][i]:\n",
    "            if 'EW' not in dataset['Type'][i]:\n",
    "                nec.append(i)\n",
    "\n",
    "        if 'ED' not in dataset['Type'][i]:\n",
    "            if 'EA' not in dataset['Type'][i]:\n",
    "                if 'ESD' not in dataset['Type'][i]:\n",
    "                    if 'EB' not in dataset['Type'][i]:\n",
    "                        ned.append(i)\n",
    "\n",
    "        if 'DSCT' not in dataset['Type'][i]:\n",
    "            if 'HADS' not in dataset['Type'][i]:\n",
    "                ndsct.append(i)\n",
    "\n",
    "        if 'DCEP' not in dataset['Type'][i]:\n",
    "            ndcep.append(i)\n",
    "        \n",
    "        if 'GDOR' not in dataset['Type'][i]:\n",
    "            ngdor.append(i)\n",
    "\n",
    "    MISC = dataset.drop(nmisc)  #Dropping all indices where the 'Type' doesn't match for each category\n",
    "    CST = dataset.drop(ncst)\n",
    "    RR = dataset.drop(nrr)\n",
    "    EC = dataset.drop(nec)\n",
    "    ED = dataset.drop(ned)\n",
    "    DSCT = dataset.drop(ndsct)\n",
    "    DCEP = dataset.drop(ndcep)\n",
    "    GDOR = dataset.drop(ngdor)\n",
    "\n",
    "    for val in MISC.Type.unique():  #Ensuring for each category that all the 'Type' names are the same for machine learning purposes\n",
    "        MISC.replace(to_replace=val, value='MISC', inplace=True)  #This could probably be consolidated into a smaller chunk of code with a bit of time\n",
    "    MISC = MISC.reset_index()\n",
    "    MISC = MISC.drop('index',axis = 1)\n",
    "\n",
    "    for val in CST.Type.unique():\n",
    "        CST.replace(to_replace=val, value='CST', inplace=True)\n",
    "    CST = CST.reset_index()\n",
    "    CST = CST.drop('index',axis = 1)\n",
    "\n",
    "    for val in RR.Type.unique():\n",
    "        RR.replace(to_replace=val, value='RR', inplace=True)\n",
    "    RR = RR.reset_index()\n",
    "    RR = RR.drop('index',axis = 1)\n",
    "\n",
    "    for val in EC.Type.unique():\n",
    "        EC.replace(to_replace=val, value='EC', inplace=True)\n",
    "    EC = EC.reset_index()\n",
    "    EC = EC.drop('index',axis = 1)\n",
    "\n",
    "    for val in ED.Type.unique():\n",
    "        ED.replace(to_replace=val, value='ED', inplace=True)\n",
    "    ED = ED.reset_index()\n",
    "    ED = ED.drop('index',axis = 1)\n",
    "\n",
    "    for val in DSCT.Type.unique():\n",
    "        DSCT.replace(to_replace=val, value='DSCT', inplace=True)\n",
    "    DSCT = DSCT.reset_index()\n",
    "    DSCT = DSCT.drop('index',axis = 1)\n",
    "\n",
    "    for val in DCEP.Type.unique():\n",
    "        DCEP.replace(to_replace=val, value='DCEP', inplace=True)\n",
    "    DCEP = DCEP.reset_index()\n",
    "    DCEP = DCEP.drop('index',axis = 1)\n",
    "\n",
    "    for val in GDOR.Type.unique():\n",
    "        GDOR.replace(to_replace=val, value='GDOR', inplace=True)\n",
    "    GDOR = GDOR.reset_index()\n",
    "    GDOR = GDOR.drop('index',axis = 1)\n",
    "\n",
    "    return(MISC,CST,RR,EC,ED,DSCT,DCEP,GDOR)  #Returns separate pandas dataframes containing only the specific type of star that correlates with the variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows you to clean a dataset that includes one \"Type\" by returning a csv file that includes the word \"yes\" or \"no\" for each index in the dataset depending on which button is pressed. When 'yes' or the y key is pressed, the lightcurve of will be saved as a jpeg file\n",
    "#Pandas \"dataset\" must have columns 'TIC_ID','Sector','Camera','CCD', and 'Type' ('Type' must all be the same type)\n",
    "#'newfilename' is the name that you want the CSV to be saved as\n",
    "#'foldername' is the name of the folder that you want the created folder of lightcurves to go into\n",
    "#'seccam' is a name for the sector and camera that the dataset is from, example 'sec15cam3'\n",
    "#max and min are the maximum and minimum period values to have a window pop up, if the period is outside of these values, 'no' will be appended to the csv file\n",
    "#Right now, only sectors 1,2,3,4,5,15,16, and 17 will work with this function\n",
    "def clean(dataset,newfilename,foldername,seccam,max,min):  \n",
    "\n",
    "    cleaned = [] #Initializing an empty list where 'yes' or 'no' will be appended depending on whether or not a light curve was labelled correctly \n",
    "\n",
    "    for i in range(len(dataset['TIC_ID'])): #Looping to get a light curve for each index in the dataset\n",
    "    \n",
    "        TICID = dataset['TIC_ID'][i]  \n",
    "        SECTOR = dataset['Sector'][i]\n",
    "        CAMERA = dataset['Camera'][i]\n",
    "        CCD = dataset['CCD'][i]\n",
    "        \n",
    "        #A different url is used to download the lightcurve data for sectors 1-5 vs sectors 15-17\n",
    "        if SECTOR in ['sector01','sector02','sector03','sector04','sector05']:  \n",
    "            url = f\"http://astro.phy.vanderbilt.edu/~oelkerrj/tess_ffi/{SECTOR}/clean/{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\"\n",
    "        if SECTOR in ['sector15','sector16','sector17']:\n",
    "            url = f\"http://astro.phy.vanderbilt.edu/~oelkerrj/tess_ffi/{SECTOR}/detrend/{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\"\n",
    "\n",
    "        os.system(f\"wget {url} -O {TICID}_{SECTOR}_{CAMERA}_{CCD}.lc > /dev/null 2>&1\") #Collecting light curve data for the index\n",
    "        data = pd.read_csv(f\"{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\",delimiter=' ',header=None,names=['time','magnitude','error'])\n",
    "        data = data.to_numpy(dtype=float)\n",
    "    \n",
    "        if np.any(np.isnan(data)) or np.any(np.isinf(data)):  #Checking if there are any nan or inf in the light curve data and skipping the index if there are\n",
    "            cleaned.append('no')\n",
    "            os.remove(f\"{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\")\n",
    "            continue\n",
    "            \n",
    "        time = data[:,0]\n",
    "        magnitude = data[:,1]\n",
    "        \n",
    "        lc = lk.LightCurve(time,magnitude).remove_outliers(sigma=5.0,return_mask = False) #\n",
    "        pg = lc.to_periodogram(minimum_period=0.025*u.day, maximum_period=20*u.day,oversample_factor=10) #Using lightkurve to find the period with max power using a lomb-scargle periodogram\n",
    "        phase = ((time-time[0])/pg.period_at_max_power.value) - np.floor((time-time[0])/pg.period_at_max_power.value)\n",
    "        phase2 = [phase-1.0,phase]\n",
    "        mag = [magnitude,magnitude]\n",
    "        \n",
    "        def plots(ax1,ax2,ax3):  #Defining a function that creates a plot with subplots: scatter plot of magnitude vs time, lomb-scargle periodogram, and a phase folded plot of the magnitude at the period at max power\n",
    "            lc.scatter(ax=ax1)\n",
    "            pg.plot(ax=ax2)\n",
    "            ax3.scatter(phase2,mag,marker = \".\",s = 5)\n",
    "\n",
    "        if pg.period_at_max_power.value > max:  #Appending 'no' to the list if the period is greater than the maximum value\n",
    "            cleaned.append('no')\n",
    "\n",
    "        elif pg.period_at_max_power.value < min: #Appending 'no' to the list if the period is less than the minimum value\n",
    "            cleaned.append('no')\n",
    "\n",
    "        else:\n",
    "        \n",
    "            window = tk.Tk()  #Creating a GUI using tkinter\n",
    "            window.title(f\"Is this a {dataset['Type'][i]}? Index {i}\")\n",
    "            window.geometry(\"500x1000\")\n",
    "\n",
    "            fig, (ax1,ax2,ax3) = plt.subplots(3,1,figsize=(18,25))\n",
    "\n",
    "            ax2.text(0.5, 0.8, f'Within (0.025,20) period = {pg.period_at_max_power}', transform=ax2.transAxes, fontsize=15)\n",
    "\n",
    "            ax1.invert_yaxis()\n",
    "            ax3.invert_yaxis()\n",
    "            ax1.set_ylabel('Magnitude')\n",
    "            ax3.set_xlabel('Phase')\n",
    "            ax3.set_ylabel('Magnitude')\n",
    "\n",
    "            plots(ax1,ax2,ax3)\n",
    "            plt.suptitle(f\"{dataset['Type'][i]}\",fontsize = 30)\n",
    "\n",
    "            canvas = FigureCanvasTkAgg(fig, master=window)\n",
    "            canvas.draw()\n",
    "\n",
    "            canvas_widget = canvas.get_tk_widget()\n",
    "            canvas_widget.pack(side=tk.TOP, fill=tk.BOTH, expand=1)\n",
    "\n",
    "            def yes_command():  #Defining a button that will append \"yes\" to the list if the button is pressed\n",
    "                cleaned.append('yes')\n",
    "                if not os.path.exists(f\"{foldername}/{seccam}_{dataset['Type'][i]}_images\"):\n",
    "                    os.makedirs(f\"{foldername}/{seccam}_{dataset['Type'][i]}_images\")\n",
    "                plt.savefig(f\"{foldername}/{seccam}_{dataset['Type'][i]}_images/{TICID}_{SECTOR}_{CAMERA}_{CCD}.jpg\")\n",
    "                plt.close(fig)\n",
    "                window.update_idletasks()\n",
    "                window.destroy()\n",
    "        \n",
    "            def no_command(): #Defining a button that will append \"no\" to the list if the button is pressed\n",
    "                cleaned.append('no')\n",
    "                plt.close(fig)\n",
    "                window.destroy()\n",
    "        \n",
    "            def on_key_press(event):  #The buttons can be finnicky so this block of code will allow the y key to be used in place of the 'yes' button and the n key in place of the 'no' button\n",
    "                if event.keysym == 'y':\n",
    "                    yes_button.invoke()\n",
    "                elif event.keysym == 'n':\n",
    "                    no_button.invoke()\n",
    "    \n",
    "            yes_button = ttk.Button(window, text=\"Yes\", command=yes_command)\n",
    "            yes_button.pack(side=tk.LEFT)\n",
    "    \n",
    "            no_button = ttk.Button(window, text=\"No\", command=no_command)\n",
    "            no_button.pack(side=tk.RIGHT)\n",
    "    \n",
    "            window.bind('<y>', on_key_press)\n",
    "            window.bind('<n>', on_key_press)\n",
    "\n",
    "            window.mainloop()\n",
    "    \n",
    "            os.remove(f\"{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\") #removing the file with the light curve data to take up less space\n",
    "        \n",
    "            if i == (len(dataset['TIC_ID'])-1):  #Creating a csv file with all the 'yes' and 'no' values\n",
    "                with open(f'{newfilename}.csv', 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    for item in cleaned:\n",
    "                        writer.writerow([item])\n",
    "    ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a function that outputs the columns that cause the highest accuracy (or silhouette for kmeans) score when put through a machine learning classifier\n",
    "#'dataset' is a pandas dataset that includes all 'types' any columns necessary for machine learning \n",
    "#cols is a list of all column names necessary for machine learning that are in the dataset\n",
    "#classifier is a machine learning classifier, example: RandomForestClassifier(random_state=20,n_estimators=25)\n",
    "#numcols is the maximum number of columns that you want to find the highest accuracy score with, recommended to choose 3 or less because otherwise it will take very long for the code to run, but 5 is the maximum\n",
    "#silhouette is either True or False depending on whether or not a silhouette score is needed instead of an accuracy score\n",
    "def bestcols(dataset,cols,classifier,numcols,silhouette):\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    dict2 = {} #Empty dictionaries are initialized for each number of columns that can be chosen\n",
    "    dict3 = {}\n",
    "    dict4 = {}\n",
    "    dict5 = {}\n",
    "\n",
    "    for val in cols: #This first chunk of code loops through combinations of 2 columns, forming a training and testing set for each\n",
    "        for val2 in cols:\n",
    "            vals2 = [val,val2]\n",
    "            if len(vals2) == len(set(vals2)) and numcols >= len(vals2):\n",
    "                X = dataset[vals2]\n",
    "                X = X.apply(pd.to_numeric)\n",
    "                Y = dataset['Type']\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.25, train_size=0.75)\n",
    "                \n",
    "                cl = classifier\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "                    cl.fit(X_train, y_train) #Each combinaiton of columns is run through the machine learning classifier\n",
    "                if silhouette == False: #If silhouette is False, accuracy score is used\n",
    "                    ypred = cl.predict(X_test)\n",
    "                    score = metrics.accuracy_score(y_test, ypred)\n",
    "                if silhouette == True: #If silhouette is True, silhouette score is used\n",
    "                    ypred = cl.fit_predict(X_test)\n",
    "                    score = silhouette_score(X_test, ypred)\n",
    "\n",
    "                dict2.update({score:vals2}) #All the scores for each combination of 2 columns is added to dict2\n",
    "                \n",
    "            for val3 in cols: #Previous comments apply to next 3 chunks of code, this one is for combinations of 3 columns and so on\n",
    "                vals3 = [val,val2,val3]\n",
    "                if len(vals3) == len(set(vals3)) and numcols >= len(vals3):\n",
    "                \n",
    "                    X = dataset[vals3]\n",
    "                    X = X.apply(pd.to_numeric)\n",
    "                    Y = dataset['Type']\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.25, train_size=0.75)\n",
    "                \n",
    "                    cl = classifier\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "                        cl.fit(X_train, y_train)\n",
    "                    if silhouette == False:\n",
    "                        ypred = cl.predict(X_test)\n",
    "                        score = metrics.accuracy_score(y_test, ypred)\n",
    "                    if silhouette == True:\n",
    "                        ypred = cl.fit_predict(X_test)\n",
    "                        score = silhouette_score(X_test, ypred)\n",
    "\n",
    "                    dict3.update({score:vals3})\n",
    "                \n",
    "                for val4 in cols:\n",
    "                    vals4 = [val,val2,val3,val4]\n",
    "                    if len(vals4) == len(set(vals4)) and numcols >= len(vals4):\n",
    "                    \n",
    "                        X = dataset[vals4]\n",
    "                        X = X.apply(pd.to_numeric)\n",
    "                        Y = dataset['Type']\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.25, train_size=0.75)\n",
    "                \n",
    "                        cl = classifier\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "                            cl.fit(X_train, y_train)\n",
    "                        if silhouette == False:\n",
    "                            ypred = cl.predict(X_test)\n",
    "                            score = metrics.accuracy_score(y_test, ypred)\n",
    "                        if silhouette == True:\n",
    "                            ypred = cl.fit_predict(X_test)\n",
    "                            score = silhouette_score(X_test, ypred)\n",
    "\n",
    "                        dict4.update({score:vals4})\n",
    "                        \n",
    "                    for val5 in cols:\n",
    "                        vals5 = [val,val2,val3,val4,val5]\n",
    "                        if len(vals4) == len(set(vals5)) and numcols >= len(vals5):\n",
    "                    \n",
    "                            X = dataset[vals5]\n",
    "                            X = X.apply(pd.to_numeric)\n",
    "                            Y = dataset['Type']\n",
    "                            X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.25, train_size=0.75)\n",
    "                \n",
    "                            cl = classifier\n",
    "                            with warnings.catch_warnings():\n",
    "                                warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "                                cl.fit(X_train, y_train)\n",
    "                            if silhouette == False:\n",
    "                                ypred = cl.predict(X_test)\n",
    "                                score = metrics.accuracy_score(y_test, ypred)\n",
    "                            if silhouette == True:\n",
    "                                ypred = cl.fit_predict(X_test)\n",
    "                                score = silhouette_score(X_test, ypred)\n",
    "\n",
    "                            dict5.update({score:vals5})\n",
    "                    \n",
    "    if numcols >= len(vals2):  #These chunks of code find the combination of 2,3,4 and 5 columns in the dictionaries that have the highest accuracy/silhouette scores, depending on what numcols is, and prints the score along with the column names\n",
    "        scores2 = list(dict2.keys())\n",
    "        maxscore2 = max(scores2)\n",
    "        cols2 = dict2[maxscore2]\n",
    "        print(maxscore2,cols2)\n",
    "    \n",
    "    if numcols >= len(vals3):           \n",
    "        scores3 = list(dict3.keys())\n",
    "        maxscore3 = max(scores3)\n",
    "        cols3 = dict3[maxscore3]\n",
    "        print(maxscore3,cols3)\n",
    "    \n",
    "    if numcols >= len(vals4):\n",
    "        scores4 = list(dict4.keys())\n",
    "        maxscore4 = max(scores4)\n",
    "        cols4 = dict4[maxscore4]\n",
    "        print(maxscore4,cols4)\n",
    "        \n",
    "    if numcols >= len(vals5):\n",
    "        scores5 = list(dict5.keys())\n",
    "        maxscore5 = max(scores5)\n",
    "        cols5 = dict5[maxscore5]\n",
    "        print(maxscore5,cols5)\n",
    "\n",
    "    X = dataset.drop(columns = ['Type']) #This runs the machine learning classifier with all columns and prints the score under the combinations of fever columns\n",
    "    X = X.apply(pd.to_numeric)\n",
    "    Y = dataset['Type']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=.25, train_size=.75)\n",
    "\n",
    "    cl = classifier\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "        cl.fit(X_train, y_train)\n",
    "    ypred = cl.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test, ypred)\n",
    "\n",
    "    print(score,\"All columns\")\n",
    "\n",
    "    end_time = datetime.now() #The time taken to run the cell is printed for reference, because with high numcols it can take a long time\n",
    "    print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function finds 'Vmag' - 'Kmag' and 'w1mag' - 'w4mag' and saves them as new columns in the dataset\n",
    "#'dataset' is a pandas dataframe that includes the columns 'Vmag','Kmag','w1mag', and 'w4mag'\n",
    "def magdiff(dataset):\n",
    "    VmKmag = [] \n",
    "    for i in range(len(dataset['Kmag'])): #For each index in the dataset, the value for Kmag is subtracted from the value for Vmag and appended to a list\n",
    "        VmKmag.append(float(dataset['Vmag'][i]) - float(dataset['Kmag'][i]))\n",
    "    \n",
    "    w1mw4mag = []\n",
    "    for i in range(len(dataset['w1mag'])): #For each index in the dataset, the value for w4mag is subtracted from the value for w1mag and appended to a list\n",
    "        w1mw4mag.append(float(dataset['w1mag'][i]) - float(dataset['w4mag'][i]))\n",
    "\n",
    "    dict = {'VmKmag': VmKmag, 'w1mw4mag': w1mw4mag} #The lists of values are given their corresponding column names in a dictionary, then converted to a pandas dataframe\n",
    "   \n",
    "    calcs = pd.DataFrame(dict)\n",
    "   \n",
    "    datasetwcalcs = pd.concat([dataset, calcs], axis=1) #The new columns 'VmKmag' and 'w1mw4mag' are added to the original dataset, then the columns 'w1mag','w4mag','Vmag','Kmag' are dropped\n",
    "    datasetwcalcs = datasetwcalcs.drop(['w1mag','w4mag','Vmag','Kmag'], axis=1)\n",
    "    return datasetwcalcs #The new dataset with the necessary magnitude differences is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function cleans a dataset that was put through the \"clean\" function by adding the CSV file returned by the \"clean\" function as a column to the dataset used, then dropping all the indices where the new column has \"no\"\n",
    "#'dataset' is a pandas dataframe that was put through the \"clean\" function, or the entire dataset with all types as long as .head() was used for the single type dataset put into the \"clean\" function\n",
    "#'type' is the star type of the dataset put through the \"clean\" function\n",
    "#'typecleanedfilename' is the name of the CSV file that was returned by the 'clean' function, containing the strings 'yes' and 'no'\n",
    "def cleaneddata(dataset,type,typecleanedfilename):\n",
    "    \n",
    "    nottypes = [] #If the entire original dataset is used as 'dataset', the indices where the 'Type' column is not equal to the type inputted will be appended to this list\n",
    "\n",
    "    if type == 'MISC':\n",
    "        for i in range(len(dataset['Type'])):\n",
    "            if 'MISC' not in dataset['Type'][i]:\n",
    "                nottypes.append(i)\n",
    "\n",
    "    if type == 'CST':\n",
    "        for i in range(len(dataset['Type'])):\n",
    "            if 'CST' not in dataset['Type'][i]:\n",
    "                nottypes.append(i)\n",
    "\n",
    "    if type == 'RR':\n",
    "        for i in range(len(dataset['Type'])):\n",
    "            if 'RR' not in dataset['Type'][i]:\n",
    "                nottypes.append(i)\n",
    "\n",
    "    if type == 'EC':\n",
    "        for i in range(len(dataset['Type'])):\n",
    "            if 'EC' not in dataset['Type'][i]:\n",
    "                if 'EW' not in dataset['Type'][i]:\n",
    "                    nottypes.append(i)\n",
    "\n",
    "    if type == 'ED':   \n",
    "        for i in range(len(dataset['Type'])):             \n",
    "            if 'ED' not in dataset['Type'][i]:\n",
    "                if 'EA' not in dataset['Type'][i]:\n",
    "                    if 'ESD' not in dataset['Type'][i]:\n",
    "                        if 'EB' not in dataset['Type'][i]:\n",
    "                            nottypes.append(i)\n",
    "\n",
    "    if type == 'DSCT': \n",
    "        for i in range(len(dataset['Type'])):\n",
    "            if 'DSCT' not in dataset['Type'][i]:\n",
    "                if 'HADS' not in dataset['Type'][i]:\n",
    "                    nottypes.append(i)\n",
    "\n",
    "    if type == 'DCEP': \n",
    "        for i in range(len(dataset['Type'])):\n",
    "            if 'DCEP' not in dataset['Type'][i]:\n",
    "                nottypes.append(i)\n",
    "\n",
    "    if type == 'GDOR': \n",
    "        for i in range(len(dataset['Type'])):     \n",
    "            if 'GDOR' not in dataset['Type'][i]:\n",
    "                nottypes.append(i)\n",
    "\n",
    "    types = dataset.drop(nottypes) #This line of code drops all indices where the \"Type\" column is not equal to the type inputted\n",
    "\n",
    "    for val in types.Type.unique():\n",
    "        types.replace(to_replace=val, value=type, inplace=True)\n",
    "\n",
    "    testing = pd.read_csv(f\"{typecleanedfilename}.csv\", sep=',',header = None) #The CSV of 'yes' and 'no' strings is loaded in as a pandas column\n",
    "\n",
    "    types = types.head(len(testing)) #If the entire dataset was used, this ensures that it is the same length as the CSV file\n",
    "    types = types.reset_index()\n",
    "    types = types.drop('index',axis = 1)\n",
    "\n",
    "    cols = pd.concat([testing, types], axis=1) #This last chunk of code concatenates the dataset and the \"yes\"/\"no\" column, and then drops the incides where the column from the CSV files says \"no\"\n",
    "    nos = []\n",
    "    for i in range(len(cols[0])):\n",
    "        if cols[0][i] == 'no':\n",
    "            nos.append(i)\n",
    "        \n",
    "    allyes = cols.drop(nos)\n",
    "    allyes = allyes.reset_index()\n",
    "    cleandata = allyes.drop('index',axis = 1)\n",
    "    \n",
    "    return cleandata #The cleaned dataset of the single star type is returned (Indices where the CSV file returned from the 'clean' function was \"yes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just a quick function that puts a dataset through a classifier, then concatenates the column of predicted types to the dataset\n",
    "#'dataset' is a pandas dataframe with any columns needed for machine learning\n",
    "#classifier is a variable name for a machine learning classifier that was previously run with a training dataset\n",
    "#cols is a list of all column names necessary for machine learning that are in the dataset\n",
    "def resulttypes(dataset,classifier,cols):\n",
    "    predtypes = classifier.predict(dataset[cols])\n",
    "    predtypes = pd.DataFrame({'PredType': predtypes})\n",
    "    \n",
    "    results = pd.concat([predtypes,dataset], axis=1)\n",
    "    return results #Returns the inputted pandas dataframe with an added column of the predicted types from the machine learning classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes a dataset that was previously returned by the 'resulttypes' function, and creates jpeg plots for each predicted type, similar to the 'clean' function\n",
    "#'dataset' is the pandas dataframe returned by the 'resulttypes' function that includes columns 'PredType','TIC_ID','Sector','Camera',and 'CCD'\n",
    "#'seccam' is a name for the sector and camera that the dataset is from, example 'sec15cam3'\n",
    "#'foldername' is the name of the main folder that you would like all the folders for each star type containing the light curves to go into.\n",
    "#Right now, only sectors 1,2,3,4,5,15,16, and 17 will work with this function\n",
    "def resultplots(dataset,seccam,foldername):\n",
    "\n",
    "    for i in range(len(dataset['TIC_ID'])): #Looping to get a light curve for each index in the dataset\n",
    "    \n",
    "        TICID = dataset['TIC_ID'][i]\n",
    "        SECTOR = dataset['Sector'][i]\n",
    "        CAMERA = dataset['Camera'][i]\n",
    "        CCD = dataset['CCD'][i]\n",
    "        if SECTOR in ['sector01','sector02','sector03','sector04','sector05']: #A different url is used to download the lightcurve data for sectors 1-5 vs sectors 15-17\n",
    "            url = f\"http://astro.phy.vanderbilt.edu/~oelkerrj/tess_ffi/{SECTOR}/clean/{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\"\n",
    "        if SECTOR in ['sector15','sector16','sector17']:\n",
    "            url = f\"http://astro.phy.vanderbilt.edu/~oelkerrj/tess_ffi/{SECTOR}/detrend/{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\"\n",
    "        \n",
    "        os.system(f\"wget {url} -O {TICID}_{SECTOR}_{CAMERA}_{CCD}.lc > /dev/null 2>&1\")\n",
    "    \n",
    "        try:\n",
    "            data = pd.read_csv(f\"{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\", delimiter=' ', header=None, names=['time', 'magnitude', 'error'])\n",
    "        except pd.errors.ParserError:\n",
    "            os.remove(f\"{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\") #Removing light curve files that cause errors so that the cell continues to run instead of being stopped\n",
    "            continue\n",
    "        data = data.to_numpy(dtype=float) #Ensuring that the datatype is float\n",
    "\n",
    "        if np.any(np.isnan(data)) or np.any(np.isinf(data)): #Checking if there are any nan or inf in the light curve data and skipping the index if there are\n",
    "            os.remove(f\"{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\")\n",
    "            continue\n",
    "            \n",
    "        time = data[:,0]\n",
    "        magnitude = data[:,1]\n",
    "\n",
    "        print(time)\n",
    "        print(magnitude)\n",
    "    \n",
    "        lc = lk.LightCurve(time,magnitude).remove_outliers(sigma=5.0,return_mask=False)\n",
    "        \n",
    "        pg = lc.to_periodogram(minimum_period=0.025*u.day, maximum_period=20*u.day,oversample_factor=10) \n",
    "        phase = ((time-time[0])/pg.period_at_max_power.value) - np.floor((time-time[0])/pg.period_at_max_power.value) #Using lightkurve to find the period with max power using a lomb-scargle periodogram\n",
    "        phase2 = np.concatenate((phase - 1.0, phase))\n",
    "        mag = np.concatenate((magnitude, magnitude))\n",
    "        \n",
    "        def plots(ax1,ax2,ax3): #Defining a function that creates a plot with subplots: scatter plot of magnitude vs time, lomb-scargle periodogram, and a phase folded plot of the magnitude at the period at max power\n",
    "            lc.scatter(ax=ax1)\n",
    "            pg.plot(ax=ax2)\n",
    "            ax3.scatter(phase2,mag,marker = \".\",s = 5)\n",
    "\n",
    "        fig, (ax1,ax2,ax3) = plt.subplots(3,1,figsize=(18,25))\n",
    "\n",
    "        ax2.text(0.5, 0.8, f'Within (0.025,20) period = {pg.period_at_max_power}', transform=ax2.transAxes, fontsize=15)\n",
    "\n",
    "        ax1.invert_yaxis()\n",
    "        ax3.invert_yaxis()\n",
    "\n",
    "        plots(ax1,ax2,ax3)\n",
    "        plt.suptitle(f\"{dataset['PredType'][i]}\",fontsize = 30)\n",
    "\n",
    "        if not os.path.exists(f\"{foldername}/{seccam}_pred_{dataset['PredType'][i]}\"):\n",
    "            os.makedirs(f\"{foldername}/{seccam}_pred_{dataset['PredType'][i]}\") #Creating a folder inside the main folder for each specific star type if the folders do not already exist\n",
    "\n",
    "        plt.suptitle(f\"{dataset['PredType'][i]}\",fontsize = 20)\n",
    "        plt.savefig(f\"{foldername}/{seccam}_pred_{dataset['PredType'][i]}/{TICID}_{SECTOR}_{CAMERA}_{CCD}.jpg\") #Saving the jpeg image of the light curve plots\n",
    "        plt.close(fig)\n",
    "    \n",
    "        os.remove(f\"{TICID}_{SECTOR}_{CAMERA}_{CCD}.lc\") #removing the file with the light curve data to take up less space\n",
    "    ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an example of how I read in the data for a sector\n",
    "#The typenames file is downloaded from VisieR (choose 'list of targets' then check the boxes for 'single table' and 'add your input as first column'. Then on the left choose 'max: '1' and ;-Separated-Values instead of HTML Table, then make sure only 'Type' is checked under simple constraint) after inputting a space separated file with 'ra' and 'dec' from filtergraph\n",
    "#The csv file for each camera is from filtergraph with the columns 'TIC_ID','Sector','Camera','CCD','Vmag','Kmag','w1mag','w4mag','Teff','Jstet','RMS_60m','LS_Period','LS_SNR','BLS_Period','BLS_SDE' checked and downloaded as a comma-separated file\n",
    "\n",
    "types1 = pd.read_csv('typenames_sector1.tsv', sep=';', comment='#',skip_blank_lines = False,header = 1)\n",
    "types1 = types1.drop([0,1])\n",
    "types1 = types1.reset_index()\n",
    "types1 = types1.drop('index',axis = 1)\n",
    "\n",
    "l = []\n",
    "for i in range(len(types1['_1'])):\n",
    "    if type(types1['_1'][i]) != str:\n",
    "        l.append(i)\n",
    "        \n",
    "types1 = types1.drop(l)\n",
    "types1 = types1.reset_index()\n",
    "cropped1 = types1.drop('index',axis = 1)\n",
    "\n",
    "cam1 = pd.read_csv('Sector1_camera1.csv', sep=',', comment='#', dtype = str)\n",
    "cam2 = pd.read_csv('Sector1_camera2.csv', sep=',', comment='#', dtype = str)\n",
    "cam3 = pd.read_csv('Sector1_camera3.csv', sep=',', comment='#', dtype = str)\n",
    "cam4 = pd.read_csv('Sector1_camera4.csv', sep=',', comment='#', dtype = str)\n",
    "cams = pd.concat([cam1,cam2,cam3,cam4])\n",
    "cams = cams.reset_index()\n",
    "alldata = cams.drop('index',axis = 1)\n",
    "alldata = alldata.drop('Unnamed: 17',axis = 1)\n",
    "\n",
    "Sector1 = pd.concat([cropped1, alldata], axis=1)\n",
    "\n",
    "m = []\n",
    "for i in range(len(Sector1['Type'])):\n",
    "    if type(Sector1['Type'][i]) != str:\n",
    "        m.append(i)\n",
    "        \n",
    "Sector1 = Sector1.drop(m)\n",
    "Sector1 = Sector1.dropna(axis=0)\n",
    "Sector1 = Sector1.reset_index()\n",
    "Sector1 = Sector1.drop('index',axis = 1)\n",
    "\n",
    "Sector1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows how the 'septypes' function separates all the star types into different pandas dataframes\n",
    "MISC1,CST1,RR1,EC1,ED1,DSCT1,DCEP1,GDOR1 = septypes(Sector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of how to use the \"clean\" function without constraining the period (just use very small and very large numbers)\n",
    "%%capture\n",
    "clean(GDOR,'GDOR_all','GDOR_images',100,0.001)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the 'magdiff' function on each of the type dataframes (This could have been done on the entire sector)\n",
    "diffCST1 = magdiff(CST1)\n",
    "diffMISC1 = magdiff(MISC1)\n",
    "diffRR1 = magdiff(RR1)\n",
    "diffEC1 = magdiff(EC1)\n",
    "diffED1 = magdiff(ED1)\n",
    "diffDSCT1 = magdiff(DSCT1)\n",
    "diffDCEP1 = magdiff(DCEP1)\n",
    "diffGDOR1 = magdiff(GDOR1)\n",
    "\n",
    "diffMISC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of cleaning each of the datasets with their corresponding CSV files from the \"clean\" function\n",
    "#GDOR and CST were more complicated because I used the \"clean\" function on MISC a couple times to find more constant and gamma doradus stars that were within the MISC category\n",
    "\n",
    "sec1cleanMISC = cleaneddata(diffMISC1,'MISC','MISCto999')\n",
    "sec1cleanRR = cleaneddata(diffRR1,'RR','RRto999')\n",
    "sec1cleanEC = cleaneddata(diffEC1,'EC','ECto999')\n",
    "sec1cleanED = cleaneddata(diffED1,'ED','EDto999')\n",
    "sec1cleanDSCT = cleaneddata(diffDSCT1,'DSCT','DSCT_all')\n",
    "sec1cleanDCEP = cleaneddata(diffDCEP1,'DCEP','DCEPto999')\n",
    "\n",
    "sec1cleanCST1 = cleaneddata(diffCST1,'CST','CST_all')\n",
    "sec1cleanGDOR1 = cleaneddata(diffGDOR1,'GDOR','GDOR_all')\n",
    "\n",
    "sec1cleanCST2 = cleaneddata(diffMISC1,'MISC','const_new')\n",
    "sec1cleanGDOR2 = cleaneddata(diffMISC1,'MISC','GDOR_new')\n",
    "\n",
    "for val in sec1cleanCST2.Type.unique():\n",
    "        sec1cleanCST2.replace(to_replace=val, value='CST', inplace=True)\n",
    "\n",
    "for val in sec1cleanGDOR2.Type.unique():\n",
    "        sec1cleanGDOR2.replace(to_replace=val, value='GDOR', inplace=True)\n",
    "\n",
    "sec1cleanGDOR = pd.concat([sec1cleanGDOR1, sec1cleanGDOR2])\n",
    "sec1cleanGDOR = sec1cleanGDOR.reset_index()\n",
    "sec1cleanGDOR = sec1cleanGDOR.drop('index',axis = 1)\n",
    "\n",
    "sec1cleanCST = pd.concat([sec1cleanCST1, sec1cleanCST2])\n",
    "sec1cleanCST = sec1cleanCST.reset_index()\n",
    "sec1cleanCST = sec1cleanCST.drop('index',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a mixed dataset of all the types concatenated to use for machine learning\n",
    "clean_sec1 = pd.concat([sec1cleanGDOR,sec1cleanDCEP,sec1cleanDSCT,sec1cleanED,sec1cleanRR,sec1cleanEC,sec1cleanMISC,sec1cleanCST])\n",
    "clean_sec1 = clean_sec1.reset_index()\n",
    "clean_sec1 = clean_sec1.drop('index',axis = 1)\n",
    "clean_sec1\n",
    "\n",
    "clean_sec1_mix = clean_sec1.sample(frac = 1)\n",
    "\n",
    "sec1_ml = clean_sec1_mix[['Type','Teff','Jstet','RMS_60m','LS_Period','LS_SNR','BLS_Period','BLS_SDE','VmKmag','w1mw4mag']]\n",
    "sec1_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of how the 'bestcols' function works\n",
    "cols = ['Teff','Jstet','RMS_60m','LS_Period','LS_SNR','BLS_Period','BLS_SDE','VmKmag','w1mw4mag']\n",
    "\n",
    "bestcols(sec1_ml,cols,RandomForestClassifier(random_state=20,n_estimators=25),3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the variable for a random forest classifier to use for 'resulttypes' function later on\n",
    "X = sec1_ml.drop('Type',axis = 1)\n",
    "X = X.apply(pd.to_numeric)\n",
    "Y = sec1_ml['Type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.3, train_size=0.7)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=30)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "ypredfor = forest.predict(X_test)\n",
    "\n",
    "score = metrics.accuracy_score(y_test, ypredfor)\n",
    "\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the classifier's confusion matrix\n",
    "metrics.confusion_matrix(y_test, ypredfor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing in sector 15 cameras 1 through 3 in a similar way to sector 1 earlier\n",
    "sec15cam1to3types = pd.read_csv('Sector15_cam1-3_typenames_07-30-24.tsv', sep=';', comment='#',skip_blank_lines = False,header = 1)\n",
    "sec15cam1to3types = sec15cam1to3types.drop([0,1])\n",
    "sec15cam1to3types = sec15cam1to3types.reset_index()\n",
    "sec15cam1to3types = sec15cam1to3types.drop('index',axis = 1)\n",
    "\n",
    "l = []\n",
    "for i in range(len(sec15cam1to3types['_1'])):\n",
    "    if type(sec15cam1to3types['_1'][i]) != str:\n",
    "        l.append(i)\n",
    "        \n",
    "sec15cam1to3types = sec15cam1to3types.drop(l)\n",
    "sec15cam1to3types = sec15cam1to3types.reset_index()\n",
    "sec15cam1to3types = sec15cam1to3types.drop('index',axis = 1)\n",
    "\n",
    "\n",
    "sec15cam3data = pd.read_csv('Sector15_camera3_07-24-24.csv', sep=',', comment='#', dtype = str)\n",
    "sec15cam1to2data = pd.read_csv('Sector15_cam1-2_07-30-24.csv', sep=',', comment='#', dtype = str)\n",
    "sec15cam1to3data = pd.concat([sec15cam1to2data,sec15cam3data])\n",
    "sec15cam1to3data = sec15cam1to3data.reset_index()\n",
    "sec15cam1to3data = sec15cam1to3data.drop('index',axis = 1)\n",
    "\n",
    "sec15cam1to3data = sec15cam1to3data.drop('Unnamed: 15',axis = 1)\n",
    "\n",
    "sec15cam1to3 = pd.concat([sec15cam1to3types, sec15cam1to3data], axis=1)\n",
    "\n",
    "m = []\n",
    "for i in range(len(sec15cam1to3['Type'])):\n",
    "    if type(sec15cam1to3['Type'][i]) != str:\n",
    "        m.append(i)\n",
    "        \n",
    "sec15cam1to3 = sec15cam1to3.drop(m)\n",
    "sec15cam1to3 = sec15cam1to3.dropna(axis=0)\n",
    "sec15cam1to3 = sec15cam1to3.drop('_1',axis = 1)\n",
    "sec15cam1to3 = sec15cam1to3.reset_index()\n",
    "sec15cam1to3 = sec15cam1to3.drop('index',axis = 1)\n",
    "\n",
    "sec15cam1to3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the columns to match the ones for sector 1\n",
    "sec15cam1to3.rename(columns={'vmag': 'Vmag','kmag': 'Kmag','teff':'Teff','Jstet_2':'Jstet','RMSBin_60.0_3':'RMS_60m','LS_Period_1_4':'LS_Period','LS_SNR_1_4':'LS_SNR','BLS_Period_1_5':'BLS_Period','BLS_SDE_1_5':'BLS_SDE'}, inplace=True)\n",
    "sec15cam1to3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using magdiff on the entire dataset\n",
    "sec15cam1to3diff = magdiff(sec15cam1to3)\n",
    "sec15cam1to3diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the 'resulttypes' function with the previously created 'forest' classifier to predict the types of the stars in\n",
    "mlcols = ['Teff','Jstet','RMS_60m','LS_Period','LS_SNR','BLS_Period','BLS_SDE','VmKmag','w1mw4mag']\n",
    "sec15cam3pred = resulttypes(sec15cam3diff,forest,mlcols)\n",
    "sec15cam3pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 'resultplots' function to plot out the predicted types with their lightcurves and save them as jpeg files for comparison\n",
    "%%capture\n",
    "resultplots(sec15cam3pred,'sec15cam3','Sector15_Camera3_Predicted')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuing on with more testing to see how accurate the 'forest' classifier is on sector 15\n",
    "#Using septypes similarly to sector 1\n",
    "MISC15,CST15,RR15,EC15,ED15,DSCT15,DCEP15,GDOR15 = septypes(sec15cam1to3diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the length of the files to see if they are different enough to affect the machine learning (77402 is way higher than 151)\n",
    "print(len(MISC15))\n",
    "print(len(CST15))\n",
    "print(len(RR15))\n",
    "print(len(EC15))\n",
    "print(len(ED15))\n",
    "print(len(DSCT15))\n",
    "print(len(DCEP15))\n",
    "print(len(GDOR15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a machine learning dataset with all indices\n",
    "sec15types = pd.concat([MISC15,CST15,RR15,EC15,ED15,DSCT15,DCEP15,GDOR15])\n",
    "sec15types_mix = sec15types.sample(frac = 1)\n",
    "sec15types_mix = sec15types_mix.reset_index()\n",
    "sec15types_mix = sec15types_mix.drop('TIC_ID',axis = 1)\n",
    "sec15types_mix = sec15types_mix.drop('Sector',axis = 1)\n",
    "sec15types_mix = sec15types_mix.drop('Camera',axis = 1)\n",
    "sec15types_mix = sec15types_mix.drop('CCD',axis = 1)\n",
    "sec15types_mix = sec15types_mix.drop('index',axis = 1)\n",
    "sec15types_mix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 150 random indices from each type to create an even dataset\n",
    "sec15typescut = pd.concat([MISC15.sample(150),CST15.sample(150),RR15.sample(150),EC15.sample(150),ED15.sample(150),DSCT15.sample(150),DCEP15.sample(150),GDOR15.sample(150)])\n",
    "sec15typescut = sec15typescut.sample(frac = 1)\n",
    "sec15typescut = sec15typescut.reset_index()\n",
    "sec15typescut = sec15typescut.drop('TIC_ID',axis = 1)\n",
    "sec15typescut = sec15typescut.drop('Sector',axis = 1)\n",
    "sec15typescut = sec15typescut.drop('Camera',axis = 1)\n",
    "sec15typescut = sec15typescut.drop('CCD',axis = 1)\n",
    "sec15typescut = sec15typescut.drop('index',axis = 1)\n",
    "sec15typescut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the best columns to use for the dataset with 150 of each type, it ends up extremely low so I think the dataset definitely needs to be cleaned\n",
    "cols = ['Teff','Jstet','RMS_60m','LS_Period','LS_SNR','BLS_Period','BLS_SDE','VmKmag','w1mw4mag']\n",
    "\n",
    "bestcols(sec15typescut,cols,RandomForestClassifier(random_state=20,n_estimators=25),3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the best columns to use for the dataset with all values of each type, I think the huge about of binaries classified is skewing the accuracy to look better than it actually is\n",
    "bestcols(sec15types_mix,cols,RandomForestClassifier(random_state=20,n_estimators=25),3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning sector 15 similarly to sector 1 using the 'clean' function for each star type\n",
    "%%capture\n",
    "clean(MISC15.head(176),'MISC15_all','Sector15_images','sec15cam1to3',100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "clean(CST15,'CST15_all','Sector15_images','sec15cam1to3',100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running into an \"Illegal type  for table item access\" error here that I still need to fix\n",
    "%%capture\n",
    "clean(DCEP15,'DCEP15_all','Sector15_images','sec15cam1to3',100,1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "clean(GDOR15,'GDOR15_all','Sector15_images','sec15cam1to3',100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "clean(RR15.head(100),'RR15to999','Sector15_images','sec15cam1to3',100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "clean(GDOR15,'GDOR15_all','Sector15_images','sec15cam1to3',100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
